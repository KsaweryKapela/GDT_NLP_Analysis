# -*- coding: utf-8 -*-
"""NLP_GDT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R94y8PfwwGC1I5NhfnBwfa2dWOFiC8ti
"""

import warnings
import pandas as pd
path = 'GDT_NLP_Analysis'

warnings.simplefilter("ignore")
file_name = '../../../../Downloads/GDT_data.xlsx'
df = pd.read_excel(io=file_name)

df['GDT_score'] = df['GDT_Matrix_1'] + df['GDT_Matrix_2'] + df['GDT_Matrix_3'] + df['GDT_Matrix_4']

df['GAD_score'] = df['GAD-7_Matrix_1'] + df['GAD-7_Matrix_2'] + df['GAD-7_Matrix_3'] + df['GAD-7_Matrix_4']\
                  + df['GAD-7_Matrix_5'] + df['GAD-7_Matrix_6'] + df['GAD-7_Matrix_7']

df['PHQ_score'] = df['PHQ_Matrix_1'] + df['PHQ_Matrix_2'] + df['PHQ_Matrix_3'] + df['PHQ_Matrix_4'] + \
                  df['PHQ_Matrix_5'] + df['PHQ_Matrix_6'] + df['PHQ_Matrix_7'] + df['PHQ_Matrix_8'] + \
                  df['PHQ_Matrix_9']

df['NLP_all'] = None

df = df.drop(0)


for i in range(len(df['NLP_all'])):
    i = i + 1
    df['NLP_all'][i] = f'{df["NLP_1"][i]} {df["NLP_2"][i]} {df["NLP_3"][i]} {df["NLP_4"][i]} {df["NLP_5"][i]} {df["NLP_6"][i]}'


model_names = {
    "herbert-klej-cased-v1": {
        "tokenizer": "allegro/herbert-klej-cased-tokenizer-v1",
        "model": "allegro/herbert-klej-cased-v1",
    },
    "herbert-base-cased": {
        "tokenizer": "allegro/herbert-base-cased",
        "model": "allegro/herbert-base-cased",
    },
    "herbert-large-cased": {
        "tokenizer": "allegro/herbert-large-cased",
        "model": "allegro/herbert-large-cased",
    },
}

import numpy as np

def NormalizeData(data):
    return (data - np.min(data)) / (np.max(data) - np.min(data))
  
df['GDT_normalized'] = NormalizeData(df['GDT_score'])

# ! pip install simpletransformers
from simpletransformers.classification import ClassificationModel, ClassificationArgs
import pandas as pd
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ! pip install sacremoses

model_args = ClassificationArgs()
model_args.num_train_epochs = 2
model_args.regression = True

model = ClassificationModel(
    "herbert",
    "allegro/herbert-base-cased",
    num_labels=1,
    args=model_args,
    use_cuda=False
)

N = 16
X_STRING = 'NLP_2'
Y_STRING = 'GDT_normalized'

x = df[X_STRING]
y = df[Y_STRING]

x_train = x[N:]
y_train = y[N:]

x_test = x[:N]
y_answers = y[:N]

train_df = pd.DataFrame({"text": x_train, "points": y_train})

# !rm -rf outputs
model.train_model(train_df)

predictions, raw_outputs = model.predict(list(x_test))

predictions = [round(item, 2) for item in predictions]
y_answers = [round(item, 2 ) for item in y_answers]
for x, y in zip(predictions, list(y_answers)):
  print(f'Predictions: {x}, answer: {y}')